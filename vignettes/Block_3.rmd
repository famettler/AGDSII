---
title: '2: Digital Soil Mapping'
author: "Fabrice Mettler"
date: "2025-11-25"
output: html_document
---

The goal of this exercise is to predict whether the soil at a depth of 100cm is waterlogged (waterlog.100). This variable is binary, taking values TRUE (1) if waterlogged, or FALSE (0) if not. A random forest is used to solve the task.

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(ranger)
library(here)
library(knitr)
library(parallel)
library(Boruta)
library(caret)
```

```{r, include = FALSE}
### loading the dataset
df_full <- readRDS("../data/df_full.rds")
df_full_1 <- readRDS(here::here("../data/df_full.rds"))

head(df_full) |>
  knitr::kable()
```

# Simple Model
We start with a simple random forest model which is trained on all the predictors.

## Model Preparation
Before training, I transform waterlog.100 into a factor to indicate a classification task:

```{r}
# transformation of the target variable into a factor
df_full <- df_full |>
  mutate(
    waterlog.100 = factor(waterlog.100))

# for confirmation
str(df_full$waterlog.100)
```

As a further step of the data preparation, I define the target (waterlog.100) and the predictor variables.

```{r}
# Specify target: The pH in the top 10cm
target <- "waterlog.100"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat(
  "The target is:", target,
  "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "..."
)
```

For the evaluation of the model, I separate the data set into a calibration and a validation set. The model is trained on the calibration data and tested on the validation data. The data set includes already labels for the split.

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> 
  filter(dataset == "calibration")

df_test <- df_full |> 
  filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> 
  drop_na()

df_test <- df_test |> 
  drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test) / n_tot) |> round(2) * 100

cat(
  "For model training, we have a calibration / validation split of: ",
  perc_cal, "/", perc_val, "%"
)
```

The data is now ready and I can implement the random forest. For that I set a seed for reproducibility. 

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```

The output confirms, that we are dealing with a classification. We further see that Mtry is 9, which is the number of randomly picked predictors per split, and the target node size is 1.
The OBB is ca. 21% which means that 21% of the observations are miss classified.

### Metrics for classification
In chapter 9.4.1.2, we learned about the metrics for classification. The confusionMatrix() function provides the important metrics. 

```{r}
# check for class balance

train_balance <- table(df_train$waterlog.100) |> prop.table()
test_balance <- table(df_test$waterlog.100) |> prop.table()
class_balance <- cbind(train_balance, test_balance)
t(class_balance)
```

Is the data balanced in terms of observed TRUE and FALSE values? What does this imply for the interpretation of the different metrics?

the class distribution shows a moderate class imbalance. Non-waterlogged soils are more frequent than waterlogged sites, with approximatel 59%  to 40% in the training set and 65% to 35% in the test set. This has an impact on the interpretation of the metrics below.

```{r}
# plot confusion matrix
pred <- predict(rf_basic, df_test)
pred_class <- pred$predictions

conf_matrix <- caret::confusionMatrix(data = pred_class, reference = df_test$waterlog.100)
conf_matrix
```
Accuracy





I visualize the confusion matrix as a mosaic plot: 
```{r}
mosaicplot(conf_matrix$table,
  main = "Confusion matrix"
)
```



# Variable Selection

### First, I plot the variable importance. Since some of them might be correlated, we need further analysis for the variable selection. Reducing variables is important to reduce model costs but also to reduce overfitting. Especially when variables are correlated, as the model may take the "important" information of one of these variables and may take the noice of the second variable into the model. 

```{r}
# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Change in OOB MSE after permutation",
    x = "",
    title = "Variable importance based on OOB"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg
```

I use the Boruta-Algorithm for variable selection

```{r}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
  y = df_train[, target],
  x = df_train[, predictors_all],
  maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
  num.threads = parallel::detectCores() - 1
)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |>
  tibble::rownames_to_column() |>
  arrange(desc(meanImp))

# plot the importance result
ggplot(
  aes(
    x = reorder(rowname, meanImp),
    y = meanImp,
    fill = decision
  ),
  data = df_bor
) +
  geom_bar(stat = "identity", width = 0.75) +
  scale_fill_manual(values = c("grey30", "tomato", "grey70")) +
  labs(
    y = "Variable importance",
    x = "",
    title = "Variable importance based on Boruta"
  ) +
  theme_classic() +
  coord_flip()
```

```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```

```{r}
# plot confusion matrix
pred_bor <- predict(rf_bor, df_test)
pred_class_bor <- pred_bor$predictions

conf_matrix_bor <- caret::confusionMatrix(data = pred_class_bor, reference = df_test$waterlog.100)
conf_matrix_bor
```

```{r}
df_mod1 <- data.frame(
  model = "Model with all predictors",
  t(c(conf_matrix$overall, conf_matrix$byClass))
  )

df_mod2 <- data.frame(
  model = "Model with selected predictors",
  t(c(conf_matrix_bor$overall, conf_matrix_bor$byClass))
  )

comparison_table <- rbind(df_mod1, df_mod2)
comparison_table <- t(comparison_table)
comparison_table

```

Which model generalises better to unseen data?
Would the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?


# Model optimization
Following hyperparameters are tuned: mtry controls how many predictors are sampled at each split, min.node.size controls the minimal size of terminal nodes.

Implementation of a 5-fold cross-validation
```{r}
set.seed(101)
mod_cv <- caret::train(df_train[, predictors_selected],
  df_train[[target]],
  method = "ranger",
  trControl = caret::trainControl(method = "cv",
                        number = 5,
                        classProbs = F),
  tuneGrid = expand.grid(mtry = c(5, 9, 15, 20),
                        splitrule = "gini",
                        min.node.size = c(1, 5, 10, 15)) ,
  importance = "permutation",
  num.trees = 500
)

# generic plot of the caret model object
ggplot(mod_cv)
```

```{r}
# generic print
print(mod_cv)
```

# Probabilistic predictions
I now implify a probabilistic model with the tuned hyperparameters. 
probability = TRUE

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_prob <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  probability = TRUE,
  mtry = 5,
  splitrule = "gini",
  min.node.size = 5,
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_prob)
```

Reicever-operating-characteristic curve
```{r}

```


The value chosen for the threshold $\tau$ is critical as it defines the point to which data points belong to the first group or the second. In the model above, we set $\tau$ to 0.5. By choosing a different value for $\tau$, we can tune the model to be more conservative to either class. In the case of informing an infrastructure construction project, where waterlogged soils are critical, one would rather choose a smaller threshold, e.g. $\tau$ = 0.9. This means that the model is 90% sure that the soil is not waterlogged. If waterlogged soil is not critical, but still unwanted, one may choose a less conservative threshold, e.g. $\tau$ = 0.75. 

It gets more complicated, when false positive outcome results in high costs. One example would be modeling the probability for a land slide. Let's imagine a small village is located below and will get hit by a landslide. One would want to inform the local authorities in time to evacuate the village. Such an evacuation on the other hand comes with great costs. A false negative outcome on the other hand would cost the life of the people in the village. In this case one would even have to value human life.
