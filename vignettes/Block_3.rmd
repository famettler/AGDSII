---
title: '2: Digital Soil Mapping'
author: "Fabrice Mettler"
date: "2025-11-25"
output:
  html_document:
    output_file: Block_3.html
---

The goal of this exercise is to predict whether the soil at a depth of 100cm is waterlogged (waterlog.100). This variable is binary, taking values TRUE (1) if waterlogged, or FALSE (0) if not. A random forest is used to solve the task.

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(ranger)
library(here)
library(knitr)
library(parallel)
library(Boruta)
library(caret)
library(pROC)
```

```{r, include=FALSE}
### loading the dataset
df_full <- readRDS(here::here("data", "df_full.rds"))

head(df_full) |>
  knitr::kable()
```
# Simple Model
We start with a simple random forest model which is trained on all the predictors.

#### Model Preparation
Before training, I transform waterlog.100 into a factor to indicate a classification task:

```{r}
# transformation of the target variable into a factor
df_full <- df_full |>
  dplyr::mutate(
    waterlog.100 = factor(
      waterlog.100,
      levels = c(0, 1),
      labels = c("no", "yes")
      )
    )

# for confirmation
str(df_full$waterlog.100)
```

As a further step of the data preparation, I define the target (waterlog.100) and the predictor variables.

```{r}
# Specify target: The pH in the top 10cm
target <- "waterlog.100"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat(
  "The target is:", target,
  "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "..."
)
```

For the evaluation of the model, I separate the data set into a calibration and a validation set. The model is trained on the calibration data and tested on the validation data. The data set includes already labels for the split.

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> 
  filter(dataset == "calibration")

df_test <- df_full |> 
  filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> 
  drop_na()

df_test <- df_test |> 
  drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test) / n_tot) |> round(2) * 100

cat(
  "For model training, we have a calibration / validation split of: ",
  perc_cal, "/", perc_val, "%"
)
```

The split into the two datasets was successful and is balanced. The data is now ready and we can implement the random forest. For that, we set a seed for reproducibility. 

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```

The output confirms, that we are dealing with a classification. We further see that Mtry is 9, which is the number of randomly picked predictors per split, and the target node size is 1.
The OOB provides an internal estimate and suggests that approximately 21% of the observations are miss-classified. 

#### Metrics for classification
In chapter 9.4.1.2, we learned about the metrics for classification. The confusionMatrix() function provides the important metrics. 

```{r}
# plot confusion matrix
pred <- predict(rf_basic, df_test)
pred_class <- pred$predictions

conf_matrix <- caret::confusionMatrix(data = pred_class,
                                      reference = df_test$waterlog.100,
                                      positive = "yes")
conf_matrix
```
```{r}
mosaicplot(conf_matrix$table,
  main = "Confusion matrix"
)
```
The results of the confusion matrix informs us about the quality of the predictions. The accuracy (i.e. the proportion of output that were correctly classified) is 77% which is pretty decent. True negatives (TN = 105), false negatives (FN = 21), false positive (FP = 25), and true positive (TP = 49) are visualized in the mosaic plot above. As soils are probably more often non-waterlogged than waterlogged, checking the class balance is important, as it influences the interpretation of the results.

```{r}
# check for class balance
train_balance <- table(df_train$waterlog.100) |> prop.table()
test_balance <- table(df_test$waterlog.100) |> prop.table()
class_balance <- cbind(train_balance, test_balance)
t(class_balance)
```

The class distribution shows a moderate class imbalance. Non-waterlogged soils are more frequent than waterlogged sites, with approximately 59% to 41% in the training set and 65% to 35% in the test set. This has an impact on the interpretation of the metrics above. The accuracy becomes less informative as the model that classifies randomly more soil as non-waterlogged has a higher probability of being right by chance. The "no information rate" is 65% and describes exactly this process. But still, the model predicts statistical significantly better than when one would just always predict the majority class (p = 0.00016). When we can not trust the accuracy, the sensitivity and specificity become more important. The sensitivity (0.70) is the "true positive rate" and describes how well the model identifies waterlogged soils. This measurement is especially important, when missing waterlogged sites has severe consequences. The specificity (0.80)), on the other hand, informs about the ability of the model to detect true negatives. The positive predict value (0.66) indicates that if the model predicts waterlogging, it is in 66% of the cases true. Similar for the negative predict value (0.83). In these metrics, we can see the imbalance, as the model performs better when predicting the major class (non-waterlogged). Finally, the balanced accuracy of 0.75 confirms that the model performs reasonably well on both classes.

# Variable Selection
First, I plot the variable importance. Since some of them might be correlated, we need further analysis for the variable selection. Reducing variables is important to reduce model costs but also to reduce overfitting. Especially when variables are correlated, as the model may take the "important" information of one of these variables and may take the noice of the second variable into the model. 

```{r, fig.height = 10, fig.width = 6}
# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Change in OOB MSE after permutation",
    x = "",
    title = "Variable importance based on OOB"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg
```
I use the Boruta-Algorithm for variable selection

```{r, fig.height = 10, fig.width = 6}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
  y = df_train[, target],
  x = df_train[, predictors_all],
  maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
  num.threads = parallel::detectCores() - 1
)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |>
  tibble::rownames_to_column() |>
  arrange(desc(meanImp))

# plot the importance result
ggplot(
  aes(
    x = reorder(rowname, meanImp),
    y = meanImp,
    fill = decision
  ),
  data = df_bor
) +
  geom_bar(stat = "identity", width = 0.75) +
  scale_fill_manual(values = c("grey30", "tomato", "grey70")) +
  labs(
    y = "Variable importance",
    x = "",
    title = "Variable importance based on Boruta"
  ) +
  theme_classic() +
  coord_flip()
```
The most important variables are NegO (Topographic negative openness), mrvbf25 (Multi-resolution valley bottom flatness at 25m resolution). Generally, the topography seems to be very important. But also mt_rr_y (Mean annual precipitation) belongs to the top 3 important predictors.

```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```
The Model with reduced predictors has a slightly improved OOB.

```{r}
# plot confusion matrix
pred_bor <- predict(rf_bor, df_test)
pred_class_bor <- pred_bor$predictions

conf_matrix_bor <- caret::confusionMatrix(data = pred_class_bor,
                                          reference = df_test$waterlog.100,
                                          positive = "yes")

df_mod1 <- data.frame(
  model = "Model with all P",
  t(c(conf_matrix$overall, conf_matrix$byClass))
  )

df_mod2 <- data.frame(
  model = "Model with sel. P",
  t(c(conf_matrix_bor$overall, conf_matrix_bor$byClass))
  )

comparison_table <- rbind(df_mod1, df_mod2)
comparison_table <- t(comparison_table)
comparison_table
```
The default of mtry is the (rounded down) square root of the number variables (=9). Default of min.node.size for classification is 1. The second model, with reduced variables, generalizes slightly better to unseen data, which is indicated in an increase in nearly all the metrics. Importantly, the sensitivity increases as well, which means that the improved generalization does not come at the cost of more false positives. When considering the OOB prediction error in the model, the outcome would not necessarily be the same. The OOB error is based on the training set and may favor more complex models which may fit the training data well but do not generalize.

# Model optimization
Following hyperparameters are tuned: mtry controls how many predictors are sampled at each split, min.node.size controls the minimal size of terminal nodes.

Implementation of a 5-fold cross-validation
```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

set.seed(101)
mod_cv <- caret::train(
  x = df_train[, predictors_selected],
  y = df_train$waterlog.100,
  method = "ranger",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = expand.grid(
    mtry = c(3, 6, 10, 20, 30),
    splitrule = "gini",
    min.node.size = c(1, 5, 10, 20)
  ),
  num.trees = 500,
  importance = "permutation"
)
ggplot(mod_cv)
```

```{r}
# generic print
print(mod_cv)
```

The model seems to perform best with a low mtry value and a minimal node size around 5. Smaller node sizes probably lead to an overfitting of the training data. We also see that the values of the ROC are close together. I implement another try with hyperparameters closer to the best hyperparameters above.

```{r}
set.seed(101)
mod_cv2 <- caret::train(
  x = df_train[, predictors_selected],
  y = df_train$waterlog.100,
  method = "ranger",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = expand.grid(
    mtry = c(3, 6, 8, 10),
    splitrule = "gini",
    min.node.size = c(3, 5, 7, 10)
  ),
  num.trees = 500,
  importance = "permutation"
)
ggplot(mod_cv2)
```

```{r}
# generic print
print(mod_cv2)
```

The ROC seems to plateau in this example and does not result in a higher ROC. I therefore use the tuned hyperparameters of the first model for the further steps.


# Probabilistic predictions
I now implify a probabilistic model with the tuned hyperparameters. 

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_prob <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  probability = TRUE,
  mtry = 3,
  splitrule = "gini",
  min.node.size = 5,
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_prob)
```
Confusion Matrix:
```{r}
# Predict class probabilities on the test data
pred_prob <- predict(rf_prob, df_test)

# Extract probabilities for the "yes" class
prob_yes <- pred_prob$predictions[, "yes"]

# Convert to binary class with threshold 0.5
pred_class_prob <- factor(
  ifelse(prob_yes >= 0.5, "yes", "no"),
  levels = c("no", "yes")  # Ensure same factor levels as reference
)

# Compute confusion matrix
conf_matrix_prob <- caret::confusionMatrix(
  data = pred_class_prob,
  reference = df_test$waterlog.100,
  positive = "yes"
)

df_mod3 <- data.frame(
  model = "Final Model",
  t(c(conf_matrix_prob$overall, conf_matrix_bor$byClass))
  )

comparison_table <- rbind(df_mod1, df_mod2, df_mod3)
comparison_table <- t(comparison_table)
comparison_table
```
The final model shows similar performance than the models before. The overall slight decrease in performance may show an unsuccessfull hyperparameter tuning which may resulted in an overfitting.

#### ROC - Receiver Operating Characteristic curve
```{r}
roc_obj <- roc(
  response = df_test$waterlog.100,  # observed classes
  predictor = prob_yes,             # predicted probabilities
  levels = c("no", "yes"),           # explicitly define class order
  direction = "<"                    # higher probs = more likely "yes"
)

plot(
  roc_obj,
  col = "black",
  lwd = 2,
  main = "ROC Curve â€“ Waterlogged Soil (100 cm)"
)
```

The value chosen for the threshold $\tau$ is critical as it defines the point to which data points belong to the first group or the second. In the model above, we set $\tau$ to 0.5. By choosing a different value for $\tau$, we can tune the model to be more conservative to either class. In the case of informing an infrastructure construction project, where waterlogged soils are critical, one would rather choose a smaller threshold, e.g. $\tau$ = 0.2. This increases sensitivity at the cost of more false positives. If waterlogged soil is not critical, but still unwanted, one may choose a less conservative threshold, e.g. $\tau$ = 0.75. 

It gets more complicated, when false positive outcome results in high costs. One example would be modeling the probability for a land slide. Let's imagine a small village is located below and would get hit by the landslide. One would want to inform the local authorities in time to evacuate the village. Such an evacuation on the other hand comes with great costs. A false negative outcome on the other hand would cost the life of the people in the village. In this case one would even have to value human life. 
