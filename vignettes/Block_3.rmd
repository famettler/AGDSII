---
title: '2: Digital Soil Mapping'
author: "Fabrice Mettler"
date: "2025-11-25"
output: html_document
---

The goal of this exercise is to predict whether the soil at a depth of 100cm is waterlogged (waterlog.100). This variable is binary, taking values TRUE (1) if waterlogged, or FALSE (0) if not. A random forest is used to solve the task.

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(ranger)
library(here)
library(knitr)
library(parallel)
library(Boruta)
library(caret)
```

```{r, include=FALSE}
### loading the dataset
df_full <- readRDS(here::here("data", "df_full.rds"))

head(df_full) |>
  knitr::kable()
```
# Simple Model
We start with a simple random forest model which is trained on all the predictors.

#### Model Preparation
Before training, I transform waterlog.100 into a factor to indicate a classification task:

```{r}
# transformation of the target variable into a factor
df_full <- df_full |>
  mutate(
    waterlog.100 = factor(waterlog.100,
    levels = c(0, 1),
    labels = c("non-waterlog", "waterlog")
    )
  )

# for confirmation
str(df_full$waterlog.100)
```

As a further step of the data preparation, I define the target (waterlog.100) and the predictor variables.

```{r}
# Specify target: The pH in the top 10cm
target <- "waterlog.100"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat(
  "The target is:", target,
  "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "..."
)
```

For the evaluation of the model, I separate the data set into a calibration and a validation set. The model is trained on the calibration data and tested on the validation data. The data set includes already labels for the split.

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> 
  filter(dataset == "calibration")

df_test <- df_full |> 
  filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> 
  drop_na()

df_test <- df_test |> 
  drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test) / n_tot) |> round(2) * 100

cat(
  "For model training, we have a calibration / validation split of: ",
  perc_cal, "/", perc_val, "%"
)
```

The split into the two datasets was successful and is balanced. The data is now ready and we can implement the random forest. For that, we set a seed for reproducibility. 

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```

The output confirms, that we are dealing with a classification. We further see that Mtry is 9, which is the number of randomly picked predictors per split, and the target node size is 1.
The OBB is ca. 21% which means that 21% of the observations are miss classified.

#### Metrics for classification
In chapter 9.4.1.2, we learned about the metrics for classification. The confusionMatrix() function provides the important metrics. 

```{r}
# plot confusion matrix
pred <- predict(rf_basic, df_test)
pred_class <- pred$predictions

conf_matrix <- caret::confusionMatrix(data = pred_class,
                                      reference = df_test$waterlog.100,
                                      positive = "waterlog")
conf_matrix
```
```{r}
mosaicplot(conf_matrix$table,
  main = "Confusion matrix"
)
```
The results of the confusion matrix informs us about the quality of the predictions. The accuracy (i.e. the proportion of output that were correctly classified) is 77% which is pretty decent. True negatives (TN = 105), false negatives (FN = 21), false positive (FP = 25), and true positive (TP = 49) are visualized in the mosaic plot above. As soils are probably more often non-waterlogged then waterlogged, checking the class balance is important, as it influences the interpretation of the results.

```{r}
# check for class balance

train_balance <- table(df_train$waterlog.100) |> prop.table()
test_balance <- table(df_test$waterlog.100) |> prop.table()
class_balance <- cbind(train_balance, test_balance)
t(class_balance)
```

The class distribution shows a moderate class imbalance. Non-waterlogged soils are more frequent than waterlogged sites, with approximately 59% to 41% in the training set and 65% to 35% in the test set. This has an impact on the interpretation of the metrics above. The accuracy becomes less informative as the model that classifies randomly more soil as non-waterlogged has a higher probability of beeing right by chance. The "no information rate" is 65% and describes exactly this process. But still, the model predicts statistical significantly better then when one would just always predict the majority class (p = 0.00016). When we can not trust the accuracy, the sensitivity and specificity become more important. The sensitivity (0.70) is the "true positive rate" and describes how well the model identifies waterlogged soils. This measurement is especially important, when missing waterlogged sites has severe consequences. The specificity (0.80)), on the other hand, informs about the ability of the model to detect true negatives. The positive predict value (0.66) indicates that if the model predicts waterlogging, it is in 66% of the cases true. Similar for the negative predict value (0.83). In these metrics, we can see the imbalance, as the model performs better when predicting the major class (non-waterlogged). Finally, the balanced accuracy of 0.75 confirms that the model perfoms reasonably well on both classes.

# Variable Selection
First, I plot the variable importance. Since some of them might be correlated, we need further analysis for the variable selection. Reducing variables is important to reduce model costs but also to reduce overfitting. Especially when variables are correlated, as the model may take the "important" information of one of these variables and may take the noice of the second variable into the model. 

```{r, fig.height = 10, fig.width = 6}
# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Change in OOB MSE after permutation",
    x = "",
    title = "Variable importance based on OOB"
  ) +
  theme_classic() +
  coord_flip()

# Display plot
gg
```

I use the Boruta-Algorithm for variable selection

```{r, fig.height = 10, fig.width = 6}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
  y = df_train[, target],
  x = df_train[, predictors_all],
  maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
  num.threads = parallel::detectCores() - 1
)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |>
  tibble::rownames_to_column() |>
  arrange(desc(meanImp))

# plot the importance result
ggplot(
  aes(
    x = reorder(rowname, meanImp),
    y = meanImp,
    fill = decision
  ),
  data = df_bor
) +
  geom_bar(stat = "identity", width = 0.75) +
  scale_fill_manual(values = c("grey30", "tomato", "grey70")) +
  labs(
    y = "Variable importance",
    x = "",
    title = "Variable importance based on Boruta"
  ) +
  theme_classic() +
  coord_flip()
```

```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```

```{r}
# plot confusion matrix
pred_bor <- predict(rf_bor, df_test)
pred_class_bor <- pred_bor$predictions

conf_matrix_bor <- caret::confusionMatrix(data = pred_class_bor,
                                          reference = df_test$waterlog.100,
                                          positive = "waterlog")

df_mod1 <- data.frame(
  model = "Model with all P",
  t(c(conf_matrix$overall, conf_matrix$byClass))
  )

df_mod2 <- data.frame(
  model = "Model with sel. P",
  t(c(conf_matrix_bor$overall, conf_matrix_bor$byClass))
  )

comparison_table <- rbind(df_mod1, df_mod2)
comparison_table <- t(comparison_table)
comparison_table
```

The second model, with reduced variables, generalizes slightly better to unseen data, which is indicated in an increase in nearly all the metrics. Importantly, the sensitivity increases as well, which means that the improved generalization does not come at the cost of more false positives. When considering the OBB prediction error in the model, the outcome would not be necessarily be the same. The OBB error is based on the training set and may favor more complex models which may fit the training data well but do not generalize.

# Model optimization
Following hyperparameters are tuned: mtry controls how many predictors are sampled at each split, min.node.size controls the minimal size of terminal nodes.

Implementation of a 5-fold cross-validation
```{r}
set.seed(101)
mod_cv <- caret::train(
  df_train[, predictors_selected],
  df_train$waterlog.100,
  method = "ranger",
  trControl = caret::trainControl(method = "cv",
                        number = 5,
                        classProbs = F),
  tuneGrid = expand.grid(mtry = c(6, 10, 20, 27),
                        splitrule = "gini",
                        min.node.size = c(1, 5, 10, 20)) ,
  importance = "permutation",
  num.trees = 500
)

# generic plot of the caret model object
ggplot(mod_cv)
```
```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

set.seed(101)
mod_cv <- caret::train(
  x = df_train[, predictors_selected],
  y = df_train$waterlog.100,
  method = "ranger",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = expand.grid(
    mtry = c(6, 10, 20, 27),
    splitrule = "gini",
    min.node.size = c(1, 5, 10, 20)
  ),
  num.trees = 500,
  importance = "permutation"
)

```


```{r}
# generic print
print(mod_cv)
```

# Probabilistic predictions
I now implify a probabilistic model with the tuned hyperparameters. 
probability = TRUE

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_prob <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 101, # Specify the seed for randomization to reproduce the same model again
  importance = "permutation", # Pick permutation to calculate variable importance
  probability = TRUE,
  mtry = 5,
  splitrule = "gini",
  min.node.size = 5,
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_prob)
```

Reicever-operating-characteristic curve
```{r}

```


The value chosen for the threshold $\tau$ is critical as it defines the point to which data points belong to the first group or the second. In the model above, we set $\tau$ to 0.5. By choosing a different value for $\tau$, we can tune the model to be more conservative to either class. In the case of informing an infrastructure construction project, where waterlogged soils are critical, one would rather choose a smaller threshold, e.g. $\tau$ = 0.9. This means that the model is 90% sure that the soil is not waterlogged. If waterlogged soil is not critical, but still unwanted, one may choose a less conservative threshold, e.g. $\tau$ = 0.75. 

It gets more complicated, when false positive outcome results in high costs. One example would be modeling the probability for a land slide. Let's imagine a small village is located below and will get hit by a landslide. One would want to inform the local authorities in time to evacuate the village. Such an evacuation on the other hand comes with great costs. A false negative outcome on the other hand would cost the life of the people in the village. In this case one would even have to value human life.
